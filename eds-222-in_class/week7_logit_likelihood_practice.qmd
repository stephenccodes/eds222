---
title: "EDS222 Week 7"
format: 
  html:
    code-tools: true
---

This week, we will review key topics from earlier weeks and postpone new material to week 8. As you complete the following exercises, please try to identify your "muddiest" areas. Are there formulas or functions you're using and can't articulate why? Are there similar topics that you're having a hard time differentiating? Make a note of these points as you go - that will help guide your studying and give me a better idea of how to clarify concepts and tools in our final weeks.

## Probability

1.  What potential values can a $Normal$ variable take? What about a $Bernoulli$ variable? **A normal vairable can take on any value, including negatives from negative infiinity to infinity. A bernoulli variable casn take on a value from 0 to 1**
    1.  What's something in the real world we could represent with a $Normal$ variable? **We could represent a value from a normal distriubution?**
    2.  How about a $Bernoulli$ variable? **We can represent the probability of an outcome**
2.  What's the difference between a probability *density* function (PDF) and a probability *mass* function (PMF)? **A PDF describes a continuous variable, while a PMF decribes the likelihood of a discrete variable.**
    1.  Consider distributions from the $Normal$ and $Bernoulli$ families. Which has a PDF? Which a PMF? **A normal variable is shown with a normal distrbution, with a PDF, while a Bernoulli varibale will be shown with a PMF.**
    2.  Consider the PDF for a $Normal(\mu=0,\sigma=1)$ variable. What's the value of the PDF when $x=0$? Does this value represent the probability of producing a 0 from this distribution? Why or why not? **The value of the PDF when x =0 will also be zero, beacuse there is no variation ion the independent variable....**
    3.  Consider the PMF for a $Bernoulli(p=0.5)$ variable. What's the value of the PMF when $x=0$? Does this value represent the probability of producing a 0 from this distribution? Why or why not?
3.  Draw the PMF for a $Bernoulli(p=0.25)$ variable by hand. Then use ggplot to recreate your drawing.

```{r}
```

4.  What's the probability of observing $\{ 0, 0, 1, 1 \}$ from a $Bernoulli(p = 0.5)$ variable? **The probability would be 0.5\^4 =.0625, because you need to get the right value four times, with a likelihood of each value being 0.5.**
    1.  Would that probability go up or down if the variable was a $Bernoulli(p = 0.99)$? **0.01 \* 0.01 \* 0.99 \* 0.99 = 9.801e-05. The probability would decrease, because the likelihood is very small for the first two values.**
5.  What's the probability of observing 0 from a $Bernoulli(p=0.1)$ AND 1 from a $Bernoulli(p=0.9)$? **They have the same probability.???**
    1.  Extending the previous question, what's the probability of observing $\{0, 0, 0, 1, 0\}$ from a $Bernoulli$ variable if $p$ is 0.1 for the first observation and increases by 0.1 for each subsequent observation? \*\*\*\* \[OBS, 0, 0, 0, 1, 0, P, 0.1, 0.2, 0.3, 0.4, 0.5, Pr, 0.9., 0.8, 0.7, 0.4, 0.5\]
6.  Describe what each following R function returns.
    1.  `dnorm()`
    2.  `pnorm()`
    3.  `rnorm()`
    4.  `dbinom()`
    5.  `rbinom()`
7.  Which R function would help you solve problem 4 above? Write the code below.

```{r}

obs <- c(0, 0, 1, 1)
prod(dbinom(obs, size = 1, prob = 0.5)) # dbinom is bernoulli, prod us like sum but for multiplying

```

## Link functions

1.  Fill in the blanks below to specify a logistic regression in statistical notation.\
    $$
    \begin{align}
    y &\sim ??? (???) \\
    ???(???) &= ??? + ??? x
    \end{align}
    $$ $$
    \begin{align}
    y &\sim Bernoulli (P) \\
    logit(P) &= \beta_0 + \beta_1 x
    \end{align}
    $$

    1.  What kind of random variable is the response?

        **The response should be represented as coordinates with values from negative infinity to infinity.**

    2.  What parameter does that variable take? **The parameter is going to be between 0 and 1.**

    3.  What transformation (link function) is applied to the parameter? **The log is taken of the parameter(probability), which gives us an output of a value from negative infinity to infinity.**

    4.  Which part of the model is linear? **The right side of the equation(beta_0 + beta_1 \* x) is linear.**

2.  What's the formula for *odds*?

    **p/ 1-p**

3.  What's the formula for the *logit* function?

    **logit(p) = beta_0 + beta_1(x)**

4.  The *logistic* function is the inverse of the logit function. Without using R or a calculator, what's the value of $logistic(logit(0))$? How about $logistic(logit(1000))$? **Logistic and logit do opposite actions, so they cancel each other out. The answers are 0 and 1000, respectively.**

    1.  What kinds of numbers can the logit function operate on? Conversely, what inputs would yield an undefined result? Why?

        **The numbers that logit can operate on are between 0 and 1,. inputs when the p = to 1 or zero would yield invalid results. This is because if you input those values into the odds equation, you are either dividing by 0or dividing 0 by 1, neither are valid.**

    2.  What about the logistic?

5.  Imagine a "logistic" regression without a link function. I.e., $p$ is linearly related to $x$. Give an example of values for $\beta_0,\beta_1,x$ that would break your regression model. What constraint on $p$ did you break? **0,1,2 would give us a p of 2, which is not a possible probability.**

6.  In your own words, describe why the logit link function is necessary for logistic regression to work.

7.  Use ggplot to create a logistic curve. Your x-axis should cover the range -5 to 5. Use $\beta_0=0,\beta_1=1$ as your coefficients.\
    Answer the following questions first using your intuition, then check your answers in code.

    1.  How will your curve change if $\beta_0$ increases to 3?
    2.  How will your curve change if you flip the sign of $\beta_1$ to -1?

## Likelihood

1.  The PMF of a random discrete variable describes the probability of observing a value given the parameters. What is the likelihood function relative to that?

    **The likelihood is equal to 1 - the probability??**

2.  Is the likelihood function directly interpretable? Or rather, in what context are likelihoods meaningful?

    \*\*There is no direct interpretation of likelihood, it is only comparable to other likelihoods\*\*

3.  Consider this logistic regression model from the week 6 lab.

    $$
    \begin{align}
    \text{healthViolation} &\sim Bernoulli(p) \\
    logit(p) &= \beta_0 + \beta_1 \text{pctHisp}
    \end{align}
    $$

    Let's say you observe the following data.

    | pctHisp | healthViolation |
    |---------|-----------------|
    | 0.00    | 1               |
    | 0.15    | 0               |
    | 0.30    | 0               |
    | 0.45    | 1               |
    | 0.60    | 1               |
    | 0.75    | 0               |
    | 0.90    | 1               |

    1.  Visualize these data using ggplot.

    2.  Let $\beta_0=0, \beta_1=1$.

        1.  Calculate $logit(p)$ across the range of $\text{pctHisp}$ values.
        2.  Calculate $p$.
        3.  Add $p$ to your ggplot as a curve. Make it red.

    3.  Let $\beta_0=-2, \beta_1=0.5$.

        1.  Repeat the calculations above and add a curve for $p$, but make it blue.

    4.  Intuitively, which curve do you think is *more likely*?

4.  Now you're going to write a likelihood function for the above logistic model.

    1.  What inputs does this function need?

    2.  What value will it return?

5.  Consider the following partially written code chunk.

    ```{r}
    #| eval: false
    inv_logit <- function(x) {
      ____
    }
    likelihood_fun <- function(coefs, data) {
      logit_p <- ____
      p <- inv_logit(____)
      loglik <- ____(____, size = 1, prob = ____, ____ = TRUE)
      -____(____)
    }
    ```

    ```{r}
            library(tidyverse)
    # 3.1
            water_data <- tibble(
              pctHisp = seq(0, 0.9, by = .15),
              healthViolation = c(1, 0, 0, 1, 1, 0, 1)
            )

            ggplot(water_data, aes(pctHisp, healthViolation)) +
              geom_point()
    ```

```{r}
# 3.2

logit <- function(p) log(p /(1-p))
inv_logit <- function(x) exp(x) / (1 + exp(x))



#beta_0 = 0, beta_1 = 1

water_data %>%
  mutate(logit_p = 0 + 1 * pctHisp,
         p = inv_logit(logit_p)) %>% 
  ggplot(aes(pctHisp, healthViolation)) +
  geom_point() +
  geom_line(aes(y = p), color = "red")
```

```{r}
#3.3
#beta_0 = -2, beta_1 = 0.5

water_data %>%
  mutate(logit_p1 = 0 + 1 * pctHisp,
         p1 = inv_logit(logit_p1),
         logit_p2 = -2 + 0.5 * pctHisp,
         p2 = inv_logit(logit_p2)) %>% 
  ggplot(aes(pctHisp, healthViolation)) +
  geom_point() +
  geom_line(aes(y = p1), color = "red") +
  geom_line(aes(y = p2), color = "blue")
```

```         
1.  What is the length of the vector `coefs` (i.e. the first parameter of the likelihood function)? What should the names of the vector be ?
2.  If `data` is a data frame, what two columns does it need to have?
3.  Why is `size = 1` important? Refer to the statistical notation above.
4.  In questions 3.2 and 3.3, you visualized $p$ for two different sets of coefficients. Use the likelihood function to calculate the negative log likelihood for both sets of coefficients. Interpret those values.
```

6.  Now you'll use optimization to estimate the *most likely* coefficients. Fill in the partially written code chunk below.

    ```{r}
    #| eval: false
    most_likely <- optim(
      par = c(____ = ____, ____ = ____),
      fn = ____,
      data = ____
    )
    ```

    1.  What estimates did you get for $\beta_0, \beta_1$?
    2.  Add the predicted values of $p$ for these coefficients to your plot with the raw data. How does this curve compare to the other two you added earlier?

7.  The last thing you'll do is create a map of the negative log likelihood "landscape". This should be raster plot with *candidate* $\beta_0$'s on the x-axis, $\beta_1$'s on the y-axis, and the fill should be the negative log likelihood. Add a point for the *most likely* coefficients. Choose your range of $\beta$'s so the most likely coefficients fall somewhere in the middle.

    ```{r}
    #| eval: false
    # This utility function will make it easier to call the likelihood function within mutate()
    likelihood_fun2 <- function(beta0, beta1, data) {
      likelihood_fun(c(beta0 = beta0, beta1 = beta1), data = data)
    }
    # Create a grid of candidate coefficients to map out the likelihood landscape
    expand_grid(
      ____ = seq(____, ____, length.out = 20),
      ____ = seq(____, ____, length.out = 20)
    ) %>%
      # Calculate the negative log likelihood for each pair of coefficients using mapply() and likelihood_fun2()
      mutate(____ = mapply(FUN = ____, 
                           ____, 
                           ____, 
                           MoreArgs = list(data = health_data))) %>% 
      # Create a "likelihood DEM"
      ggplot() +
      geom_raster(aes(x = ____, y = ____, fill = ____)) +
      # Most likely coefficients
      geom_point(x = ____, y = ____, color = "red") +
      # Make the fill of the likelihood raster cells look like terrain
      scale_fill_stepsn(colors = terrain.colors(6)) +
      theme_bw()
    ```

    1.  Are your most likely coefficients down in a valley or up on a peak? Why?
    2.  Pick three pairs of coefficients that fall in green, yellow/orange, and pink/white parts of the likelihood landscape. Recreate the plots from question 3 (raw data with predicted $p$ curve) using those three sets of coefficients. Qualitatively describe the shapes of the curves, where the coefficients fall in likelihood landscape, and how well the curves match the data.
